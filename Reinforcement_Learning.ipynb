{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtS/c5WvaONrPhQBT9s2If",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishwanath103/MIT-DL-Course/blob/main/Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v6lWagjiSfy",
        "outputId": "7dad2390-a674-489a-de6d-13eadd2aa954"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl x11-utils > /dev/null 2>&1\r\n",
        "!pip install gym pyvirtualdisplay scikit-video > /dev/null 2>&1\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import base64, io, time, gym\r\n",
        "import IPython, functools\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import time\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "!pip install mitdeeplearning\r\n",
        "import mitdeeplearning as mdl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mitdeeplearning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/ad/650eb53c0d9d1213536fe94bc150f89b564ff5ee784bd662272584bb091b/mitdeeplearning-0.2.0.tar.gz (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mitdeeplearning) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from mitdeeplearning) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mitdeeplearning) (4.41.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from mitdeeplearning) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->mitdeeplearning) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->mitdeeplearning) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->mitdeeplearning) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->mitdeeplearning) (0.16.0)\n",
            "Building wheels for collected packages: mitdeeplearning\n",
            "  Building wheel for mitdeeplearning (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mitdeeplearning: filename=mitdeeplearning-0.2.0-cp37-none-any.whl size=2115442 sha256=8dfe124a401cdac8200499188d3e5e20b755001b067adb8d302a00ca8dfaf5a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/dc/2a/5c3633135e7e4ef4fd31463cfa1942cb1bae7486ab94e7a2ad\n",
            "Successfully built mitdeeplearning\n",
            "Installing collected packages: mitdeeplearning\n",
            "Successfully installed mitdeeplearning-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlJph5-Lj11P"
      },
      "source": [
        "# Part 1: Cartpole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe7rFV4_j6QY"
      },
      "source": [
        "## Define the Cartpole environment and agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxW6iFE2lVhe"
      },
      "source": [
        "### Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69VOvwSwjco2",
        "outputId": "76a4ac44-108a-4a52-f905-79fd4b7bc0d5"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\r\n",
        "env.seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaADHkwXkunM"
      },
      "source": [
        "Our observations in this Cartpole environment are:\r\n",
        "1. Cart position\r\n",
        "2. Cart Velocity\r\n",
        "3. Pole angle\r\n",
        "4. Pole rotation rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ys9EAwkEwS",
        "outputId": "9bc8a622-0991-4d2e-d2b1-dc7f6374a572"
      },
      "source": [
        "n_observations = env.observation_space\r\n",
        "print(\"Environment has observation space = \", n_observations)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment has observation space =  Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWGJRqthke7h",
        "outputId": "d076c2a7-eb81-4d3b-b65b-c46d86015109"
      },
      "source": [
        "n_actions = env.action_space.n\r\n",
        "print(\"Number of possible actions that the agent can choose from =\", n_actions)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of possible actions that the agent can choose from = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3JT5ooglYWF"
      },
      "source": [
        "### Cartpole agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg7VEfoWlQiw"
      },
      "source": [
        "### Define the Cartpole agent ###\r\n",
        "\r\n",
        "# Defines a feed-forward neural network\r\n",
        "def create_cartpole_model():\r\n",
        "  model = tf.keras.models.Sequential([\r\n",
        "        # First Dense layer\r\n",
        "        tf.keras.layers.Dense(units=32, activation='relu'),\r\n",
        "        # Define the last Dense layer, which will provide the network's output\r\n",
        "        tf.keras.layers.Dense(units=n_actions, activation=None)\r\n",
        "  ])\r\n",
        "  return model\r\n",
        "\r\n",
        "cartpole_model = create_cartpole_model()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjL1yINOma84"
      },
      "source": [
        "### Define the agents action function ###\r\n",
        "\r\n",
        "# Fucntion that takes observations as input, executes a forward pass through model,\r\n",
        "#   and outputs a sampled action\r\n",
        "# Arguments:\r\n",
        "#   model: the network that defines our agent\r\n",
        "#   observation: observation(s) which is/are fed as input to the model\r\n",
        "#   single: flag as to whether we are handling a single observation or batch of\r\n",
        "#     observation, provided as an np.array\r\n",
        "# Returns:\r\n",
        "#   action: choice of agent action\r\n",
        "def choose_action(model, observation, single=True):\r\n",
        "  # add batch dimension to the observation if only a single example was provided\r\n",
        "  observation = np.expand_dims(observation, axis=0) if single else observation\r\n",
        "\r\n",
        "  # Feed the observations through the model to predict the log probabilities of each posible action\r\n",
        "  logits = model.predict(observation)\r\n",
        "\r\n",
        "  # Choose an action\r\n",
        "  action = tf.random.categorical(logits, num_samples=1)\r\n",
        "  action = action.numpy().flatten()\r\n",
        "  return action[0] if single else action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjzpUKYzof6K"
      },
      "source": [
        "## Define the agents memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0RE7-roocer"
      },
      "source": [
        "### Agent Memory ###\r\n",
        "\r\n",
        "class Memory:\r\n",
        "  def __init__(self):\r\n",
        "    self.clear()\r\n",
        "\r\n",
        "  # Resets/restarts the memory buffer\r\n",
        "  def clear(self):\r\n",
        "    self.observation = []\r\n",
        "    self.actions = []\r\n",
        "    self.rewards = []\r\n",
        "\r\n",
        "  # Add observations, actions, rewards to memory\r\n",
        "  def add_to_memory(self, new_observation, new_action, new_reward):\r\n",
        "    self.observations.append(new_observation)\r\n",
        "    self.actions.append(new_action)\r\n",
        "    self.rewards.append(new_reward)\r\n",
        "\r\n",
        "# Helper function to combine a list of Memory objects into a single Memory.\r\n",
        "def aggregate_memories(memories):\r\n",
        "  batch_memory = Memory()\r\n",
        "\r\n",
        "  for memory in memories:\r\n",
        "    for step in zip(memory.observations, memory.actions, memory.rewards):\r\n",
        "      batch_memory.add_to_memory(*step)\r\n",
        "\r\n",
        "  return batch_memory\r\n",
        "\r\n",
        "# Instantiate a single Memory buffer\r\n",
        "memory = Memory()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loV6C5Gmqkz3"
      },
      "source": [
        "## Reward Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0l3rvhWqeQg"
      },
      "source": [
        "### Reward function ###\r\n",
        "\r\n",
        "# Helper function that normalizes an np.array x\r\n",
        "def normalize(x):\r\n",
        "  x -= np.mean(x)\r\n",
        "  x /= np.std(x)\r\n",
        "  return x.astype(np.float32)\r\n",
        "\r\n",
        "# Compute normalized, discounted, cumulative rewards\r\n",
        "# Arguments:\r\n",
        "#   rewards: reward at timesteps in episode\r\n",
        "#   gamma: discounting factor\r\n",
        "# Returns:\r\n",
        "#   normalized discounted reward\r\n",
        "def discount_rewards(rewards, gamma=0.95):\r\n",
        "  discounted_rewards = np.zeros_like(rewards)\r\n",
        "  R = 0\r\n",
        "  for t in reversed(range(0, len(rewards))):\r\n",
        "    # update the total discounted reward\r\n",
        "    R = R * gamma + rewards[t]\r\n",
        "    discounted_rewards[t] = R\r\n",
        "\r\n",
        "  return normalize(discounted_rewards)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJAxqtTJtDm4"
      },
      "source": [
        "## Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4qtOzzLtAM2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}